{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESUrb0gGR6TQ"
   },
   "source": [
    "# Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nonrjto2UmGa",
    "outputId": "abd3a561-e901-4048-c8fd-46554a56e598"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DjdFJfERbevh",
    "outputId": "2a247200-5401-4bc3-9aff-7743c2443a69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.12\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "MS0-_Nad_1Eu",
    "outputId": "c3e703fb-c1b4-46ab-c651-727bb83b33ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository: 'deb https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu/ jammy main'\n",
      "Description:\n",
      "This PPA contains more recent Python versions packaged for Ubuntu.\n",
      "\n",
      "Disclaimer: there's no guarantee of timely updates in case of security problems or other issues. If you want to use them in a security-or-otherwise-critical environment (say, on a production server), you do so at your own risk.\n",
      "\n",
      "Update Note\n",
      "===========\n",
      "Please use this repository instead of ppa:fkrull/deadsnakes.\n",
      "\n",
      "Reporting Issues\n",
      "================\n",
      "\n",
      "Issues can be reported in the master issue tracker at:\n",
      "https://github.com/deadsnakes/issues/issues\n",
      "\n",
      "Supported Ubuntu and Python Versions\n",
      "====================================\n",
      "\n",
      "- Ubuntu 20.04 (focal) Python3.5 - Python3.7, Python3.9 - Python3.13\n",
      "- Ubuntu 22.04 (jammy) Python3.7 - Python3.9, Python3.11 - Python3.13\n",
      "- Ubuntu 24.04 (noble) Python3.7 - Python3.11, Python3.13\n",
      "- Note: Python2.7 (focal, jammy), Python 3.8 (focal), Python 3.10 (jammy), Python3.12 (noble) are not provided by deadsnakes as upstream ubuntu provides those packages.\n",
      "\n",
      "Why some packages aren't built:\n",
      "- Note: for focal, older python versions require libssl<1.1 so they are not currently built\n",
      "- Note: for jammy and noble, older python versions requre libssl<3 so they are not currently built\n",
      "- If you need these, reach out to asottile to set up a private ppa\n",
      "\n",
      "The packages may also work on other versions of Ubuntu or Debian, but that is not tested or supported.\n",
      "\n",
      "Packages\n",
      "========\n",
      "\n",
      "The packages provided here are loosely based on the debian upstream packages with some modifications to make them more usable as non-default pythons and on ubuntu.  As such, the packages follow debian's patterns and often do not include a full python distribution with just `apt install python#.#`.  Here is a list of packages that may be useful along with the default install:\n",
      "\n",
      "- `python#.#-dev`: includes development headers for building C extensions\n",
      "- `python#.#-venv`: provides the standard library `venv` module\n",
      "- `python#.#-distutils`: provides the standard library `distutils` module\n",
      "- `python#.#-lib2to3`: provides the `2to3-#.#` utility as well as the standard library `lib2to3` module\n",
      "- `python#.#-gdbm`: provides the standard library `dbm.gnu` module\n",
      "- `python#.#-tk`: provides the standard library `tkinter` module\n",
      "\n",
      "Third-Party Python Modules\n",
      "==========================\n",
      "\n",
      "Python modules in the official Ubuntu repositories are packaged to work with the Python interpreters from the official repositories. Accordingly, they generally won't work with the Python interpreters from this PPA. As an exception, pure-Python modules for Python 3 will work, but any compiled extension modules won't.\n",
      "\n",
      "To install 3rd-party Python modules, you should use the common Python packaging tools.  For an introduction into the Python packaging ecosystem and its tools, refer to the Python Packaging User Guide:\n",
      "https://packaging.python.org/installing/\n",
      "\n",
      "Sources\n",
      "=======\n",
      "The package sources are available at:\n",
      "https://github.com/deadsnakes/\n",
      "\n",
      "Nightly Builds\n",
      "==============\n",
      "\n",
      "For nightly builds, see ppa:deadsnakes/nightly https://launchpad.net/~deadsnakes/+archive/ubuntu/nightly\n",
      "More info: https://launchpad.net/~deadsnakes/+archive/ubuntu/ppa\n",
      "Adding repository.\n",
      "Found existing deb entry in /etc/apt/sources.list.d/deadsnakes-ubuntu-ppa-jammy.list\n",
      "Adding deb entry to /etc/apt/sources.list.d/deadsnakes-ubuntu-ppa-jammy.list\n",
      "Found existing deb-src entry in /etc/apt/sources.list.d/deadsnakes-ubuntu-ppa-jammy.list\n",
      "Adding disabled deb-src entry to /etc/apt/sources.list.d/deadsnakes-ubuntu-ppa-jammy.list\n",
      "Adding key to /etc/apt/trusted.gpg.d/deadsnakes-ubuntu-ppa.gpg with fingerprint F23C5A6CF475977595C89F51BA6932366A755776\n",
      "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
      "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
      "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
      "Get:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
      "Get:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
      "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
      "Get:11 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,331 kB]\n",
      "Get:12 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,458 kB]\n",
      "Get:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,226 kB]\n",
      "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,626 kB]\n",
      "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,535 kB]\n",
      "Get:16 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [32.9 kB]\n",
      "Get:17 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [49.7 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,514 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,453 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,741 kB]\n",
      "Fetched 26.4 MB in 10s (2,632 kB/s)\n",
      "Reading package lists... Done\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
      "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
      "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
      "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
      "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
      "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
      "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Reading package lists... Done\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "Note, selecting 'libcasa-python3-6' for regex 'python3.6'\n",
      "Note, selecting 'libpython3.6-stdlib' for regex 'python3.6'\n",
      "Note, selecting 'python3.6-2to3' for regex 'python3.6'\n",
      "The following additional packages will be installed:\n",
      "  libcasa-casa6\n",
      "The following NEW packages will be installed:\n",
      "  libcasa-casa6 libcasa-python3-6\n",
      "0 upgraded, 2 newly installed, 0 to remove and 55 not upgraded.\n",
      "Need to get 1,088 kB of archives.\n",
      "After this operation, 4,238 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcasa-casa6 amd64 3.4.0-2build1 [1,000 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcasa-python3-6 amd64 3.4.0-2build1 [88.2 kB]\n",
      "Fetched 1,088 kB in 1s (810 kB/s)\n",
      "Selecting previously unselected package libcasa-casa6:amd64.\n",
      "(Reading database ... 123632 files and directories currently installed.)\n",
      "Preparing to unpack .../libcasa-casa6_3.4.0-2build1_amd64.deb ...\n",
      "Unpacking libcasa-casa6:amd64 (3.4.0-2build1) ...\n",
      "Selecting previously unselected package libcasa-python3-6:amd64.\n",
      "Preparing to unpack .../libcasa-python3-6_3.4.0-2build1_amd64.deb ...\n",
      "Unpacking libcasa-python3-6:amd64 (3.4.0-2build1) ...\n",
      "Setting up libcasa-casa6:amd64 (3.4.0-2build1) ...\n",
      "Setting up libcasa-python3-6:amd64 (3.4.0-2build1) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
      "\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "E: Unable to locate package python3.6-dev\n",
      "E: Couldn't find any package by glob 'python3.6-dev'\n",
      "E: Couldn't find any package by regex 'python3.6-dev'\n",
      "--2024-12-10 01:22:34--  https://bootstrap.pypa.io/get-pip.py\n",
      "Resolving bootstrap.pypa.io (bootstrap.pypa.io)... 151.101.0.175, 151.101.64.175, 151.101.128.175, ...\n",
      "Connecting to bootstrap.pypa.io (bootstrap.pypa.io)|151.101.0.175|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2275758 (2.2M) [text/x-python]\n",
      "Saving to: ‘get-pip.py’\n",
      "\n",
      "get-pip.py          100%[===================>]   2.17M  --.-KB/s    in 0.05s   \n",
      "\n",
      "2024-12-10 01:22:34 (40.8 MB/s) - ‘get-pip.py’ saved [2275758/2275758]\n",
      "\n",
      "/bin/bash: line 1: python3.6: command not found\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "!add-apt-repository --yes ppa:deadsnakes/ppa\n",
    "!apt-get update\n",
    "!apt-get install python3.6\n",
    "!apt-get install python3.6-dev\n",
    "\n",
    "!wget https://bootstrap.pypa.io/get-pip.py && python3.6 get-pip.py\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path[2] = '/usr/lib/python36.zip'\n",
    "sys.path[3] = '/usr/lib/python3.6'\n",
    "sys.path[4] = '/usr/lib/python3.6/lib-dynload'\n",
    "sys.path[5] = '/usr/local/lib/python3.6/dist-packages'\n",
    "sys.path[7] = '/usr/local/lib/python3.6/dist-packages/IPython/extensions'\n",
    "\n",
    "#clear_output()\n",
    "#time.sleep(1)\n",
    "\n",
    "#import os\n",
    "#os.kill(os.getpid(), 9)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "TtObwRpxcSpE",
    "outputId": "14390c41-8df2-4351-9ed4-4d1252c027f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.16.1\n",
      "  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (18.1.1)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow==2.16.1)\n",
      "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (4.25.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.68.1)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow==2.16.1)\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.0.0 (from tensorflow==2.16.1)\n",
      "  Downloading keras-3.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (0.37.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.23.5)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.16.1) (0.45.1)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow==2.16.1) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow==2.16.1) (0.0.8)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow==2.16.1) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow==2.16.1) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow==2.16.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow==2.16.1) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow==2.16.1) (0.1.2)\n",
      "Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.7.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: ml-dtypes, tensorboard, keras, tensorflow\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml-dtypes 0.4.1\n",
      "    Uninstalling ml-dtypes-0.4.1:\n",
      "      Successfully uninstalled ml-dtypes-0.4.1\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.12.3\n",
      "    Uninstalling tensorboard-2.12.3:\n",
      "      Successfully uninstalled tensorboard-2.12.3\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.12.0\n",
      "    Uninstalling keras-2.12.0:\n",
      "      Successfully uninstalled keras-2.12.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.12.0\n",
      "    Uninstalling tensorflow-2.12.0:\n",
      "      Successfully uninstalled tensorflow-2.12.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.16.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed keras-3.7.0 ml-dtypes-0.3.2 tensorboard-2.16.2 tensorflow-2.16.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "e8e4656283a34b40be0344faa922e2a4",
       "pip_warning": {
        "packages": [
         "keras",
         "ml_dtypes",
         "tensorboard",
         "tensorflow"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nfp in /usr/local/lib/python3.10/dist-packages (0.3.12)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nfp) (1.23.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nfp) (4.66.6)\n",
      "Requirement already satisfied: networkx>2.0 in /usr/local/lib/python3.10/dist-packages (from nfp) (3.4.2)\n",
      "Requirement already satisfied: rdkit-pypi in /usr/local/lib/python3.10/dist-packages (2022.9.5)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit-pypi) (1.23.5)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit-pypi) (11.0.0)\n",
      "Requirement already satisfied: rdkit-pypi in /usr/local/lib/python3.10/dist-packages (2022.9.5)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit-pypi) (1.23.5)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit-pypi) (11.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3070, in _dep_map\n",
      "    return self.__dep_map\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2863, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _DistInfoDistribution__dep_map\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
      "    status = run_func(*args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 447, in run\n",
      "    conflicts = self._determine_conflicts(to_install)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 578, in _determine_conflicts\n",
      "    return check_install_conflicts(to_install)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
      "    package_set, _ = create_package_set_from_installed()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
      "    dependencies = list(dist.iter_dependencies())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 247, in iter_dependencies\n",
      "    return self._dist.requires(extras)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2786, in requires\n",
      "    dm = self._dep_map\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3072, in _dep_map\n",
      "    self.__dep_map = self._compute_dependencies()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3094, in _compute_dependencies\n",
      "    dm[s_extra] = [r for r in reqs_for_extra(extra) if r not in common]\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3094, in <listcomp>\n",
      "    dm[s_extra] = [r for r in reqs_for_extra(extra) if r not in common]\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3086, in reqs_for_extra\n",
      "    if not req.marker or req.marker.evaluate({'extra': extra}):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/markers.py\", line 310, in evaluate\n",
      "    current_environment = cast(\"dict[str, str]\", default_environment())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/markers.py\", line 249, in default_environment\n",
      "    \"platform_machine\": platform.machine(),\n",
      "  File \"/usr/lib/python3.10/platform.py\", line 947, in machine\n",
      "    return uname().machine\n",
      "  File \"/usr/lib/python3.10/platform.py\", line 814, in uname\n",
      "    def uname():\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1732, in isEnabledFor\n",
      "    return self._cache[level]\n",
      "KeyError: 50\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
      "    return command.main(cmd_args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
      "    return self._main(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
      "    return run(options, args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 215, in exc_logging_wrapper\n",
      "    logger.critical(\"Operation cancelled by user\")\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1523, in critical\n",
      "    if self.isEnabledFor(CRITICAL):\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1743, in isEnabledFor\n",
      "    _releaseLock()\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 233, in _releaseLock\n",
      "    _lock.release()\n",
      "KeyboardInterrupt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.16.1\n",
    "!pip install nfp\n",
    "!pip install rdkit-pypi\n",
    "!pip install --upgrade rdkit-pypi pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZXE_6i8R9VH"
   },
   "source": [
    "## - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FkjzR-DaOGPU"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "#from tensorflow import set_random_seed\n",
    "#set_random_seed(2)\n",
    "\n",
    "\n",
    "#from nfp.preprocessing import MolPreprocessor, GraphSequence\n",
    "\n",
    "import gzip\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Define Keras model\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras import (activations, initializers, regularizers, constraints)\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler\n",
    "\n",
    "from keras.layers import (Input, Embedding, Dense, BatchNormalization, Dropout,\n",
    "                                 Concatenate, Multiply, Add, Layer, Lambda)\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "from keras.utils import Sequence\n",
    "\n",
    "#from nfp.layers import (MessageLayer, GRUStep, Squeeze, EdgeNetwork,\n",
    " #                              ReduceAtomToMol, ReduceBondToAtom,\n",
    "  #                             GatherAtomToBond, ReduceAtomToPro)\n",
    "#from nfp.models import GraphModel\n",
    "import argparse\n",
    "\n",
    "\n",
    "from nfp.preprocessing import features\n",
    "from nfp.preprocessing import Tokenizer\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "from rdkit import Chem\n",
    "\n",
    "import tensorflow as tf\n",
    "from random import shuffle\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPvAXXmTR_Xt"
   },
   "source": [
    "## - Getting classes from the CASCADE paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2MgA2m0WQIQ2"
   },
   "outputs": [],
   "source": [
    "def atomic_number_tokenizer(atom):\n",
    "    return atom.GetAtomicNum()\n",
    "def Mol_iter(df):\n",
    "    for index,r in df.iterrows():\n",
    "        yield(r['Mol'], r['atom_index'])\n",
    "\n",
    "class SmilesPreprocessor(object):\n",
    "    \"\"\" Given a list of SMILES strings, encode these molecules as atom and\n",
    "    connectivity feature matricies.\n",
    "\n",
    "    Example:\n",
    "    >>> preprocessor = SmilesPreprocessor(explicit_hs=False)\n",
    "    >>> inputs = preprocessor.fit(data.smiles)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, explicit_hs=True, atom_features=None, bond_features=None):\n",
    "        \"\"\"\n",
    "\n",
    "        explicit_hs : bool\n",
    "            whether to tell RDkit to add H's to a molecule.\n",
    "        atom_features : function\n",
    "            A function applied to an rdkit.Atom that returns some\n",
    "            representation (i.e., string, integer) for the Tokenizer class.\n",
    "        bond_features : function\n",
    "            A function applied to an rdkit Bond to return some description.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.atom_tokenizer = Tokenizer()\n",
    "        self.bond_tokenizer = Tokenizer()\n",
    "        self.explicit_hs = explicit_hs\n",
    "\n",
    "        if atom_features is None:\n",
    "            atom_features = features.atom_features_v1\n",
    "\n",
    "        if bond_features is None:\n",
    "            bond_features = features.bond_features_v1\n",
    "\n",
    "        self.atom_features = atom_features\n",
    "        self.bond_features = bond_features\n",
    "\n",
    "\n",
    "    def fit(self, smiles_iterator):\n",
    "        \"\"\" Fit an iterator of SMILES strings, creating new atom and bond\n",
    "        tokens for unseen molecules. Returns a dictionary with 'atom' and\n",
    "        'connectivity' entries \"\"\"\n",
    "        return list(self.preprocess(smiles_iterator, train=True))\n",
    "\n",
    "\n",
    "    def predict(self, smiles_iterator):\n",
    "        \"\"\" Uses previously determined atom and bond tokens to convert a SMILES\n",
    "        iterator into 'atom' and 'connectivity' matrices. Ensures that atom and\n",
    "        bond classes commute with previously determined results. \"\"\"\n",
    "        return list(self.preprocess(smiles_iterator, train=False))\n",
    "\n",
    "\n",
    "    def preprocess(self, smiles_iterator, train=True):\n",
    "\n",
    "        self.atom_tokenizer.train = train\n",
    "        self.bond_tokenizer.train = train\n",
    "\n",
    "        for smiles in tqdm(smiles_iterator):\n",
    "            yield self.construct_feature_matrices(smiles)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def atom_classes(self):\n",
    "        \"\"\" The number of atom types found (includes the 0 null-atom type) \"\"\"\n",
    "        return self.atom_tokenizer.num_classes + 1\n",
    "\n",
    "\n",
    "    @property\n",
    "    def bond_classes(self):\n",
    "        \"\"\" The number of bond types found (includes the 0 null-bond type) \"\"\"\n",
    "        return self.bond_tokenizer.num_classes + 1\n",
    "\n",
    "\n",
    "    def construct_feature_matrices(self, smiles):\n",
    "        \"\"\" construct a molecule from the given smiles string and return atom\n",
    "        and bond classes.\n",
    "\n",
    "        Returns\n",
    "        dict with entries\n",
    "        'n_atom' : number of atoms in the molecule\n",
    "        'n_bond' : number of bonds in the molecule\n",
    "        'atom' : (n_atom,) length list of atom classes\n",
    "        'bond' : (n_bond,) list of bond classes\n",
    "        'connectivity' : (n_bond, 2) array of source atom, target atom pairs.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        mol = MolFromSmiles(smiles)\n",
    "        if self.explicit_hs:\n",
    "            mol = AddHs(mol)\n",
    "\n",
    "        n_atom = len(mol.GetAtoms())\n",
    "        n_bond = 2 * len(mol.GetBonds())\n",
    "\n",
    "        # If its an isolated atom, add a self-link\n",
    "        if n_bond == 0:\n",
    "            n_bond = 1\n",
    "\n",
    "        atom_feature_matrix = np.zeros(n_atom, dtype='int')\n",
    "        bond_feature_matrix = np.zeros(n_bond, dtype='int')\n",
    "        connectivity = np.zeros((n_bond, 2), dtype='int')\n",
    "\n",
    "        bond_index = 0\n",
    "\n",
    "        atom_seq = mol.GetAtoms()\n",
    "        atoms = [atom_seq[i] for i in range(n_atom)]\n",
    "\n",
    "        for n, atom in enumerate(atoms):\n",
    "\n",
    "            # Atom Classes\n",
    "            atom_feature_matrix[n] = self.atom_tokenizer(\n",
    "                self.atom_features(atom))\n",
    "\n",
    "            start_index = atom.GetIdx()\n",
    "\n",
    "            for bond in atom.GetBonds():\n",
    "                # Is the bond pointing at the target atom\n",
    "                rev = bond.GetBeginAtomIdx() != start_index\n",
    "\n",
    "                # Bond Classes\n",
    "                bond_feature_matrix[n] = self.bond_tokenizer(\n",
    "                    self.bond_features(bond, flipped=rev))\n",
    "\n",
    "                # Connectivity\n",
    "                if not rev:  # Original direction\n",
    "                    connectivity[bond_index, 0] = bond.GetBeginAtomIdx()\n",
    "                    connectivity[bond_index, 1] = bond.GetEndAtomIdx()\n",
    "\n",
    "                else:  # Reversed\n",
    "                    connectivity[bond_index, 0] = bond.GetEndAtomIdx()\n",
    "                    connectivity[bond_index, 1] = bond.GetBeginAtomIdx()\n",
    "\n",
    "                bond_index += 1\n",
    "\n",
    "\n",
    "        return {\n",
    "            'n_atom': n_atom,\n",
    "            'n_bond': n_bond,\n",
    "            'atom': atom_feature_matrix,\n",
    "            'bond': bond_feature_matrix,\n",
    "            'connectivity': connectivity,\n",
    "        }\n",
    "\n",
    "\n",
    "class ConnectivityAPreprocessor(object):\n",
    "    \"\"\" Given a list of SMILES strings, encode these molecules as atom and\n",
    "    connectivity feature matricies.\n",
    "\n",
    "    Example:\n",
    "    >>> preprocessor = SmilesPreprocessor(explicit_hs=False)\n",
    "    >>> inputs = preprocessor.fit(data.smiles)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, explicit_hs=True, atom_features=None, bond_features=None):\n",
    "        \"\"\"\n",
    "\n",
    "        explicit_hs : bool\n",
    "            whether to tell RDkit to add H's to a molecule.\n",
    "        atom_features : function\n",
    "            A function applied to an rdkit.Atom that returns some\n",
    "            representation (i.e., string, integer) for the Tokenizer class.\n",
    "        bond_features : function\n",
    "            A function applied to an rdkit Bond to return some description.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.atom_tokenizer = Tokenizer()\n",
    "        self.bond_tokenizer = Tokenizer()\n",
    "        self.explicit_hs = explicit_hs\n",
    "\n",
    "        if atom_features is None:\n",
    "            atom_features = features.atom_features_v1\n",
    "\n",
    "        if bond_features is None:\n",
    "            bond_features = features.bond_features_v1\n",
    "\n",
    "        self.atom_features = atom_features\n",
    "        self.bond_features = bond_features\n",
    "\n",
    "\n",
    "    def fit(self, smiles_iterator):\n",
    "        \"\"\" Fit an iterator of SMILES strings, creating new atom and bond\n",
    "        tokens for unseen molecules. Returns a dictionary with 'atom' and\n",
    "        'connectivity' entries \"\"\"\n",
    "        return list(self.preprocess(smiles_iterator, train=True))\n",
    "\n",
    "\n",
    "    def predict(self, smiles_iterator):\n",
    "        \"\"\" Uses previously determined atom and bond tokens to convert a SMILES\n",
    "        iterator into 'atom' and 'connectivity' matrices. Ensures that atom and\n",
    "        bond classes commute with previously determined results. \"\"\"\n",
    "        return list(self.preprocess(smiles_iterator, train=False))\n",
    "\n",
    "\n",
    "    def preprocess(self, smiles_iterator, train=True):\n",
    "\n",
    "        self.atom_tokenizer.train = train\n",
    "        self.bond_tokenizer.train = train\n",
    "\n",
    "        for smiles in tqdm(smiles_iterator):\n",
    "            yield self.construct_feature_matrices(smiles)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def atom_classes(self):\n",
    "        \"\"\" The number of atom types found (includes the 0 null-atom type) \"\"\"\n",
    "        return self.atom_tokenizer.num_classes + 1\n",
    "\n",
    "\n",
    "    @property\n",
    "    def bond_classes(self):\n",
    "        \"\"\" The number of bond types found (includes the 0 null-bond type) \"\"\"\n",
    "        return self.bond_tokenizer.num_classes + 1\n",
    "\n",
    "\n",
    "    def construct_feature_matrices(self, smiles):\n",
    "        \"\"\" construct a molecule from the given smiles string and return atom\n",
    "        and bond classes.\n",
    "\n",
    "        Returns\n",
    "        dict with entries\n",
    "        'n_atom' : number of atoms in the molecule\n",
    "        'n_bond' : number of bonds in the molecule\n",
    "        'atom' : (n_atom,) length list of atom classes\n",
    "        'bond' : (n_bond,) list of bond classes\n",
    "        'connectivity' : (n_bond, 2) array of source atom, target atom pairs.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        mol = MolFromSmiles(smiles)\n",
    "        if self.explicit_hs:\n",
    "            mol = AddHs(mol)\n",
    "\n",
    "        n_atom = len(mol.GetAtoms())\n",
    "        n_bond = 2 * len(mol.GetBonds())\n",
    "\n",
    "        # If its an isolated atom, add a self-link\n",
    "        if n_bond == 0:\n",
    "            n_bond = 1\n",
    "\n",
    "        atom_feature_matrix = np.zeros(n_atom, dtype='int')\n",
    "        bond_feature_matrix = np.zeros(n_bond, dtype='int')\n",
    "        connectivity = np.zeros((n_bond, 2), dtype='int')\n",
    "\n",
    "        bond_index = 0\n",
    "\n",
    "        atom_seq = mol.GetAtoms()\n",
    "        atoms = [atom_seq[i] for i in range(n_atom)]\n",
    "\n",
    "        for n, atom in enumerate(atoms):\n",
    "\n",
    "            # Atom Classes\n",
    "            atom_feature_matrix[n] = self.atom_tokenizer(\n",
    "                self.atom_features(atom))\n",
    "\n",
    "            start_index = atom.GetIdx()\n",
    "\n",
    "            for bond in atom.GetBonds():\n",
    "                # Is the bond pointing at the target atom\n",
    "                rev = bond.GetBeginAtomIdx() != start_index\n",
    "\n",
    "                # Bond Classes\n",
    "                bond_feature_matrix[n] = self.bond_tokenizer(\n",
    "                    self.bond_features(bond, flipped=rev))\n",
    "\n",
    "                # Connectivity\n",
    "                if not rev:  # Original direction\n",
    "                    connectivity[bond_index, 0] = bond.GetBeginAtomIdx()\n",
    "                    connectivity[bond_index, 1] = bond.GetEndAtomIdx()\n",
    "\n",
    "                else:  # Reversed\n",
    "                    connectivity[bond_index, 0] = bond.GetEndAtomIdx()\n",
    "                    connectivity[bond_index, 1] = bond.GetBeginAtomIdx()\n",
    "\n",
    "                bond_index += 1\n",
    "\n",
    "        return {\n",
    "            'n_atom': n_atom,\n",
    "            'n_bond': n_bond,\n",
    "            'atom': atom_feature_matrix,\n",
    "            'bond': bond_feature_matrix,\n",
    "            'connectivity': connectivity,\n",
    "        }\n",
    "\n",
    "\n",
    "class MolPreprocessor(SmilesPreprocessor):\n",
    "    \"\"\" I should refactor this into a base class and separate\n",
    "    SmilesPreprocessor classes. But the idea is that we only need to redefine\n",
    "    the `construct_feature_matrices` method to have a working preprocessor that\n",
    "    handles 3D structures.\n",
    "\n",
    "    We'll pass an iterator of mol objects instead of SMILES strings this time,\n",
    "    though.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_neighbors, cutoff, **kwargs):\n",
    "        \"\"\" A preprocessor class that also returns distances between\n",
    "        neighboring atoms. Adds edges for non-bonded atoms to include a maximum\n",
    "        of n_neighbors around each atom \"\"\"\n",
    "\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.cutoff = cutoff\n",
    "        super(MolPreprocessor, self).__init__(**kwargs)\n",
    "\n",
    "\n",
    "    def construct_feature_matrices(self, mol):\n",
    "        \"\"\" Given an rdkit mol, return atom feature matrices, bond feature\n",
    "        matrices, and connectivity matrices.\n",
    "\n",
    "        Returns\n",
    "        dict with entries\n",
    "        'n_atom' : number of atoms in the molecule\n",
    "        'n_bond' : number of edges (likely n_atom * n_neighbors)\n",
    "        'atom' : (n_atom,) length list of atom classes\n",
    "        'bond' : (n_bond,) list of bond classes. 0 for no bond\n",
    "        'distance' : (n_bond,) list of bond distances\n",
    "        'connectivity' : (n_bond, 2) array of source atom, target atom pairs.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        n_atom = len(mol.GetAtoms())\n",
    "\n",
    "        # n_bond is actually the number of atom-atom pairs, so this is defined\n",
    "        # by the number of neighbors for each atom.\n",
    "        #if there is cutoff,\n",
    "        distance_matrix = Chem.Get3DDistanceMatrix(mol)\n",
    "\n",
    "        if self.n_neighbors <= (n_atom - 1):\n",
    "            n_bond = self.n_neighbors * n_atom\n",
    "        else:\n",
    "            # If there are fewer atoms than n_neighbors, all atoms will be\n",
    "            # connected\n",
    "            n_bond = distance_matrix[(distance_matrix < self.cutoff) & (distance_matrix != 0)].size\n",
    "\n",
    "        if n_bond == 0: n_bond = 1\n",
    "\n",
    "        # Initialize the matrices to be filled in during the following loop.\n",
    "        atom_feature_matrix = np.zeros(n_atom, dtype='int')\n",
    "        bond_feature_matrix = np.zeros(n_bond, dtype='int')\n",
    "        bond_distance_matrix = np.zeros(n_bond, dtype=np.float32)\n",
    "        connectivity = np.zeros((n_bond, 2), dtype='int')\n",
    "\n",
    "        # Hopefully we've filtered out all problem mols by now.\n",
    "        if mol is None:\n",
    "            raise RuntimeError(\"Issue in loading mol\")\n",
    "\n",
    "        # Get a list of the atoms in the molecule.\n",
    "        atom_seq = mol.GetAtoms()\n",
    "        atoms = [atom_seq[i] for i in range(n_atom)]\n",
    "\n",
    "        # Here we loop over each atom, and the inner loop iterates over each\n",
    "        # neighbor of the current atom.\n",
    "        bond_index = 0  # keep track of our current bond.\n",
    "        for n, atom in enumerate(atoms):\n",
    "\n",
    "            # update atom feature matrix\n",
    "            atom_feature_matrix[n] = self.atom_tokenizer(\n",
    "                self.atom_features(atom))\n",
    "\n",
    "            # if n_neighbors is greater than total atoms, then each atom is a\n",
    "            # neighbor.\n",
    "            if (self.n_neighbors + 1) > len(mol.GetAtoms()):\n",
    "                neighbor_end_index = len(mol.GetAtoms())\n",
    "            else:\n",
    "                neighbor_end_index = (self.n_neighbors + 1)\n",
    "\n",
    "            distance_atom = distance_matrix[n, :]\n",
    "            cutoff_end_index = distance_atom[distance_atom < self.cutoff].size\n",
    "\n",
    "            end_index = min(neighbor_end_index, cutoff_end_index)\n",
    "\n",
    "            # Loop over each of the nearest neighbors\n",
    "\n",
    "            neighbor_inds = distance_matrix[n, :].argsort()[1:end_index]\n",
    "            if len(neighbor_inds)==0: neighbor_inds = [n]\n",
    "            for neighbor in neighbor_inds:\n",
    "\n",
    "                # update bond feature matrix\n",
    "                bond = mol.GetBondBetweenAtoms(n, int(neighbor))\n",
    "                if bond is None:\n",
    "                    bond_feature_matrix[bond_index] = 0\n",
    "                else:\n",
    "                    rev = False if bond.GetBeginAtomIdx() == n else True\n",
    "                    bond_feature_matrix[bond_index] = self.bond_tokenizer(\n",
    "                        self.bond_features(bond, flipped=rev))\n",
    "\n",
    "                distance = distance_matrix[n, neighbor]\n",
    "                bond_distance_matrix[bond_index] = distance\n",
    "\n",
    "                # update connectivity matrix\n",
    "                connectivity[bond_index, 0] = n\n",
    "                connectivity[bond_index, 1] = neighbor\n",
    "\n",
    "                bond_index += 1\n",
    "        print(connectivity)\n",
    "\n",
    "        return {\n",
    "            'n_atom': n_atom,\n",
    "            'n_bond': n_bond,\n",
    "            'atom': atom_feature_matrix,\n",
    "            'bond': bond_feature_matrix,\n",
    "            'distance': bond_distance_matrix,\n",
    "            'connectivity': connectivity,\n",
    "        }\n",
    "\n",
    "\n",
    "class MolBPreprocessor(MolPreprocessor):\n",
    "    \"\"\"\n",
    "    This is a subclass of Molpreprocessor that preprocessor molecule with\n",
    "    bond property target\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        A preprocessor class that also returns bond_target_matrix, besides the bond matrix\n",
    "        returned by MolPreprocessor. The bond_target_matrix is then used as ref to reduce molecule\n",
    "        to bond property\n",
    "        \"\"\"\n",
    "        super(MolBPreprocessor, self).__init__(**kwargs)\n",
    "\n",
    "    def construct_feature_matrices(self, entry):\n",
    "        \"\"\"\n",
    "        Given an entry contining rdkit molecule, bond_index and for the target property,\n",
    "        return atom\n",
    "        feature matrices, bond feature matrices, distance matrices, connectivity matrices and bond\n",
    "        ref matrices.\n",
    "\n",
    "        returns\n",
    "        dict with entries\n",
    "        see MolPreproccessor\n",
    "        'bond_index' : ref array to the bond index\n",
    "        \"\"\"\n",
    "        mol, bond_index_array = entry\n",
    "\n",
    "        n_atom = len(mol.GetAtoms())\n",
    "        n_pro = len(bond_index_array)\n",
    "\n",
    "        # n_bond is actually the number of atom-atom pairs, so this is defined\n",
    "        # by the number of neighbors for each atom.\n",
    "        #if there is cutoff,\n",
    "        distance_matrix = Chem.Get3DDistanceMatrix(mol)\n",
    "\n",
    "        if self.n_neighbors <= (n_atom - 1):\n",
    "            n_bond = self.n_neighbors * n_atom\n",
    "        else:\n",
    "            # If there are fewer atoms than n_neighbors, all atoms will be\n",
    "            # connected\n",
    "            n_bond = distance_matrix[(distance_matrix < self.cutoff) & (distance_matrix != 0)].size\n",
    "\n",
    "        if n_bond == 0: n_bond = 1\n",
    "\n",
    "        # Initialize the matrices to be filled in during the following loop.\n",
    "        atom_feature_matrix = np.zeros(n_atom, dtype='int')\n",
    "        bond_feature_matrix = np.zeros(n_bond, dtype='int')\n",
    "        bond_distance_matrix = np.zeros(n_bond, dtype=np.float32)\n",
    "        bond_index_matrix = np.full(n_bond, -1, dtype='int')\n",
    "        connectivity = np.zeros((n_bond, 2), dtype='int')\n",
    "\n",
    "        # Hopefully we've filtered out all problem mols by now.\n",
    "        if mol is None:\n",
    "            raise RuntimeError(\"Issue in loading mol\")\n",
    "\n",
    "        # Get a list of the atoms in the molecule.\n",
    "        atom_seq = mol.GetAtoms()\n",
    "        atoms = [atom_seq[i] for i in range(n_atom)]\n",
    "\n",
    "        # Here we loop over each atom, and the inner loop iterates over each\n",
    "        # neighbor of the current atom.\n",
    "        bond_index = 0  # keep track of our current bond.\n",
    "        for n, atom in enumerate(atoms):\n",
    "            # update atom feature matrix\n",
    "            atom_feature_matrix[n] = self.atom_tokenizer(\n",
    "                self.atom_features(atom))\n",
    "\n",
    "            # if n_neighbors is greater than total atoms, then each atom is a\n",
    "            # neighbor.\n",
    "            if (self.n_neighbors + 1) > len(mol.GetAtoms()):\n",
    "                neighbor_end_index = len(mol.GetAtoms())\n",
    "            else:\n",
    "                neighbor_end_index = (self.n_neighbors + 1)\n",
    "\n",
    "            distance_atom = distance_matrix[n, :]\n",
    "            cutoff_end_index = distance_atom[distance_atom < self.cutoff].size\n",
    "\n",
    "            end_index = min(neighbor_end_index, cutoff_end_index)\n",
    "\n",
    "            # Loop over each of the nearest neighbors\n",
    "\n",
    "            neighbor_inds = distance_matrix[n, :].argsort()[1:end_index]\n",
    "            if len(neighbor_inds)==0: neighbor_inds = [n]\n",
    "            for neighbor in neighbor_inds:\n",
    "\n",
    "                # update bond feature matrix\n",
    "                bond = mol.GetBondBetweenAtoms(n, int(neighbor))\n",
    "                if bond is None:\n",
    "                    bond_feature_matrix[bond_index] = 0\n",
    "                else:\n",
    "                    rev = False if bond.GetBeginAtomIdx() == n else True\n",
    "                    bond_feature_matrix[bond_index] = self.bond_tokenizer(\n",
    "                        self.bond_features(bond, flipped=rev))\n",
    "                    try:\n",
    "                        bond_index_matrix[bond_index] = bond_index_array.tolist().index(bond.GetIdx())\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                distance = distance_matrix[n, neighbor]\n",
    "                bond_distance_matrix[bond_index] = distance\n",
    "\n",
    "                # update connectivity matrix\n",
    "                connectivity[bond_index, 0] = n\n",
    "                connectivity[bond_index, 1] = neighbor\n",
    "\n",
    "                bond_index += 1\n",
    "        return {\n",
    "            'n_atom': n_atom,\n",
    "            'n_bond': n_bond,\n",
    "            'n_pro': n_pro,\n",
    "            'atom': atom_feature_matrix,\n",
    "            'bond': bond_feature_matrix,\n",
    "            'distance': bond_distance_matrix,\n",
    "            'connectivity': connectivity,\n",
    "            'bond_index': bond_index_matrix,\n",
    "        }\n",
    "\n",
    "class MolAPreprocessor(MolPreprocessor):\n",
    "    \"\"\"\n",
    "    This is a subclass of Molpreprocessor that preprocessor molecule with\n",
    "    bond property target\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        A preprocessor class that also returns bond_target_matrix, besides the bond matrix\n",
    "        returned by MolPreprocessor. The bond_target_matrix is then used as ref to reduce molecule\n",
    "        to bond property\n",
    "        \"\"\"\n",
    "        super(MolAPreprocessor, self).__init__(**kwargs)\n",
    "\n",
    "    def construct_feature_matrices(self, entry):\n",
    "        \"\"\"\n",
    "        Given an entry contining rdkit molecule, bond_index and for the target property,\n",
    "        return atom\n",
    "        feature matrices, bond feature matrices, distance matrices, connectivity matrices and bond\n",
    "        ref matrices.\n",
    "\n",
    "        returns\n",
    "        dict with entries\n",
    "        see MolPreproccessor\n",
    "        'bond_index' : ref array to the bond index\n",
    "        \"\"\"\n",
    "        mol, atom_index_array = entry\n",
    "\n",
    "        n_atom = len(mol.GetAtoms())\n",
    "        n_pro = len(atom_index_array)\n",
    "\n",
    "        # n_bond is actually the number of atom-atom pairs, so this is defined\n",
    "        # by the number of neighbors for each atom.\n",
    "        #if there is cutoff,\n",
    "        distance_matrix = Chem.Get3DDistanceMatrix(mol)\n",
    "\n",
    "        #if self.n_neighbors <= (n_atom - 1):\n",
    "        #    n_bond = self.n_neighbors * n_atom\n",
    "        #else:\n",
    "            # If there are fewer atoms than n_neighbors, all atoms will be\n",
    "            # connected\n",
    "        n_bond = distance_matrix[(distance_matrix < self.cutoff) & (distance_matrix != 0)].size\n",
    "\n",
    "        if n_bond == 0: n_bond = 1\n",
    "\n",
    "        # Initialize the matrices to be filled in during the following loop.\n",
    "        atom_feature_matrix = np.zeros(n_atom, dtype='int')\n",
    "        bond_feature_matrix = np.zeros(n_bond, dtype='int')\n",
    "        bond_distance_matrix = np.zeros(n_bond, dtype=np.float32)\n",
    "        atom_index_matrix = np.full(n_atom, -1, dtype='int')\n",
    "        connectivity = np.zeros((n_bond, 2), dtype='int')\n",
    "\n",
    "        # Hopefully we've filtered out all problem mols by now.\n",
    "        if mol is None:\n",
    "            raise RuntimeError(\"Issue in loading mol\")\n",
    "\n",
    "        # Get a list of the atoms in the molecule.\n",
    "        atom_seq = mol.GetAtoms()\n",
    "        atoms = [atom_seq[i] for i in range(n_atom)]\n",
    "\n",
    "        # Here we loop over each atom, and the inner loop iterates over each\n",
    "        # neighbor of the current atom.\n",
    "        bond_index = 0  # keep track of our current bond.\n",
    "        for n, atom in enumerate(atoms):\n",
    "            # update atom feature matrix\n",
    "            atom_feature_matrix[n] = self.atom_tokenizer(\n",
    "                self.atom_features(atom))\n",
    "            try:\n",
    "                atom_index_matrix[n] = atom_index_array.tolist().index(atom.GetIdx())\n",
    "            except:\n",
    "                pass\n",
    "            # if n_neighbors is greater than total atoms, then each atom is a\n",
    "            # neighbor.\n",
    "            if (self.n_neighbors + 1) > len(mol.GetAtoms()):\n",
    "                neighbor_end_index = len(mol.GetAtoms())\n",
    "            else:\n",
    "                neighbor_end_index = (self.n_neighbors + 1)\n",
    "\n",
    "            distance_atom = distance_matrix[n, :]\n",
    "            cutoff_end_index = distance_atom[distance_atom < self.cutoff].size\n",
    "\n",
    "            end_index = min(neighbor_end_index, cutoff_end_index)\n",
    "\n",
    "            # Loop over each of the nearest neighbors\n",
    "\n",
    "            neighbor_inds = distance_matrix[n, :].argsort()[1:end_index]\n",
    "            if len(neighbor_inds)==0: neighbor_inds = [n]\n",
    "            for neighbor in neighbor_inds:\n",
    "\n",
    "                # update bond feature matrix\n",
    "                bond = mol.GetBondBetweenAtoms(n, int(neighbor))\n",
    "                try:\n",
    "                    if bond is None:\n",
    "                        bond_feature_matrix[bond_index] = 0\n",
    "                    else:\n",
    "                        rev = False if bond.GetBeginAtomIdx() == n else True\n",
    "                        bond_feature_matrix[bond_index] = self.bond_tokenizer(\n",
    "                            self.bond_features(bond, flipped=rev))\n",
    "                except:\n",
    "                    print('AAAAAAAAAAAAAAA')\n",
    "                    print(mol.GetProp('_Name'))\n",
    "                    print(mol.GetProp('ConfId'))\n",
    "\n",
    "                distance = distance_matrix[n, neighbor]\n",
    "                bond_distance_matrix[bond_index] = distance\n",
    "\n",
    "                # update connectivity matrix\n",
    "                connectivity[bond_index, 0] = n\n",
    "                connectivity[bond_index, 1] = neighbor\n",
    "\n",
    "                bond_index += 1\n",
    "        return {\n",
    "            'n_atom': n_atom,\n",
    "            'n_bond': n_bond,\n",
    "            'n_pro': n_pro,\n",
    "            'atom': atom_feature_matrix,\n",
    "            'bond': bond_feature_matrix,\n",
    "            'distance': bond_distance_matrix,\n",
    "            'connectivity': connectivity,\n",
    "            'atom_index': atom_index_matrix,\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eNLSbWHnUPbl"
   },
   "source": [
    "# More Class imports from the paper (There's a lot of code review to do D:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "WWPFyuKXQ6DR"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GraphModel(Model):\n",
    "    \"\"\" This is a simple modification of the Keras `Model` class to avoid\n",
    "    checking each input for a consistent batch_size dimension. Should work as\n",
    "    of keras-team/keras#11548.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def _standardize_user_data(self, *args, **kwargs):\n",
    "        kwargs['check_array_lengths'] = False\n",
    "        return super(GraphModel, self)._standardize_user_data(*args, **kwargs)\n",
    "class MessageLayer(Layer):\n",
    "    \"\"\" Implements the matrix multiplication message functions from Gilmer\n",
    "    2017. This could probably be implemented as a series of other layers, but\n",
    "    this is more convenient.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dropout=0., reducer=None, **kwargs):\n",
    "        \"\"\"\n",
    "\n",
    "        dropout : float between 0 and 1\n",
    "            Whether to apply dropout to individual messages before they are\n",
    "            reduced to each incoming atom.\n",
    "\n",
    "        reducer : ['sum', 'mean', 'max', or 'min']\n",
    "            How to collect incoming messages for each atom. In this library,\n",
    "            I'm careful to only have messages be a function of the sending\n",
    "            atom, so we can sort the connectivity matrix by recieving atom.\n",
    "            That lets us use the `segment_*` methods from tensorflow, instead\n",
    "            of the `unsorted_segment_*` methods.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.reducer = reducer\n",
    "\n",
    "        reducer_dict = {\n",
    "            None: tf.segment_sum,\n",
    "            'sum': tf.segment_sum,\n",
    "            'mean': tf.segment_mean,\n",
    "            'max': tf.segment_max,\n",
    "            'min': tf.segment_min\n",
    "        }\n",
    "\n",
    "        self._reducer = reducer_dict[reducer]\n",
    "\n",
    "        super(MessageLayer, self).__init__(**kwargs)\n",
    "\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        \"\"\" Perform a single message passing step, returing the summed messages\n",
    "        for each recieving atom.\n",
    "\n",
    "        Inputs are [atom_matrix, bond_matrix, connectivity_matrix]\n",
    "\n",
    "        atom_matrix : (num_atoms_in_batch, d)\n",
    "            The input matrix of current hidden states for each atom\n",
    "\n",
    "        bond_matrix : (num_bonds_in_batch, d, d)\n",
    "            A matrix of current edge features, with each edge represented as a\n",
    "            (dxd) matrix.\n",
    "\n",
    "        connectivity : (num_bonds_in_batch, 2)\n",
    "            A matrix of (a_i, a_j) pairs that indicates the bond in bond_matrix\n",
    "            connecting atom_matrix[a_j] to atom_matrix[a_i].\n",
    "            The first entry indicates the recieving atom.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        atom_matrix, bond_matrix, connectivity = inputs\n",
    "\n",
    "        # Gather the atom matrix so that each reciever node corresponds with\n",
    "        # the given bond_matrix entry\n",
    "        atom_gathered = tf.gather(atom_matrix, connectivity[:, 1])\n",
    "\n",
    "        # Multiply the bond matrices by the gathered atom matrices\n",
    "        messages = K.batch_dot(bond_matrix, atom_gathered)\n",
    "\n",
    "        # Add dropout on a message-by-message basis if desired\n",
    "        def add_dropout():\n",
    "            if 0. < self.dropout < 1.:\n",
    "                return K.dropout(messages, self.dropout)\n",
    "            else:\n",
    "                return messages\n",
    "\n",
    "        dropout_messages = K.in_train_phase(\n",
    "            add_dropout(), messages, training=training)\n",
    "\n",
    "        # Sum each message along the (sorted) reciever nodes\n",
    "        summed_message = self._reducer(dropout_messages, connectivity[:, 0])\n",
    "\n",
    "        return summed_message\n",
    "\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\" Computes the shape of the output, which should be the same\n",
    "        dimension as the first input, that atom hidden state \"\"\"\n",
    "\n",
    "        assert input_shape and len(input_shape) == 3\n",
    "        assert input_shape[0][-1]  # atom hidden state dimension must be specified\n",
    "        return input_shape[0]\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'dropout': self.dropout,\n",
    "            'reducer': self.reducer,\n",
    "        }\n",
    "        base_config = super(MessageLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class GatherAtomToBond(Layer):\n",
    "    \"\"\" Reshapes the atom matrix (num_atoms_in_batch, d) to the bond matrix\n",
    "    (num_bonds_in_batch, d) by reindexing according to which atom is involved\n",
    "    in each bond.\n",
    "\n",
    "    index : 0 or 1\n",
    "        whether to gather the sending atoms (1) or recieving atoms (0) for each\n",
    "        bond.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, index, **kwargs):\n",
    "        self.index = index\n",
    "        super(GatherAtomToBond, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        atom_matrix, connectivity = inputs\n",
    "        return  tf.gather(atom_matrix, connectivity[:, self.index])\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\" Computes the shape of the output,\n",
    "        which should be the shape of the atom matrix with the length\n",
    "        of the bond matrix \"\"\"\n",
    "\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'index': self.index,\n",
    "        }\n",
    "        base_config = super(GatherAtomToBond, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "class Reducer(Layer):\n",
    "    \"\"\" Superclass for reducing methods.\n",
    "\n",
    "    reducer : ['sum', 'mean', 'max', or 'min']\n",
    "        How to collect elements for each atom or molecule. In this library,\n",
    "        I'm careful to only have messages be a function of the sending\n",
    "        atom, so we can sort the connectivity matrix by recieving atom.\n",
    "        That lets us use the `segment_*` methods from tensorflow, instead\n",
    "        of the `unsorted_segment_*` methods.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, reducer=None, **kwargs):\n",
    "\n",
    "        self.reducer = reducer\n",
    "\n",
    "        reducer_dict = {\n",
    "            None: tf.math.segment_sum,\n",
    "            'sum': tf.math.segment_sum,\n",
    "            'unsorted_sum': tf.math.unsorted_segment_sum,\n",
    "            'mean': tf.math.segment_mean,\n",
    "            'unsorted_mean': tf.math.unsorted_segment_mean,\n",
    "            'max': tf.math.segment_max,\n",
    "            'min': tf.math.segment_min\n",
    "        }\n",
    "\n",
    "        self._reducer = reducer_dict[reducer]\n",
    "\n",
    "        super(Reducer, self).__init__(**kwargs)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'reducer': self.reducer}\n",
    "        base_config = super(Reducer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        # Output shape is (n_graphs, atom_dim)\n",
    "        return input_shape[0]\n",
    "\n",
    "\n",
    "class ReduceAtomToMol(Reducer):\n",
    "    \"\"\" Sum over all atoms in each molecule.\n",
    "\n",
    "    Inputs\n",
    "\n",
    "    atom_matrix : (num_atoms_in_batch, d)\n",
    "        atom hidden states for each atom in the batch\n",
    "\n",
    "    node_graph_indices : (num_atoms_in_batch,)\n",
    "        A scalar for each atom representing which molecule in the batch the\n",
    "        atom belongs to. This is generated by the preprocessor class, and\n",
    "        essentially looks like [0, 0, 0, 1, 1] for a batch with a 3 atom\n",
    "        molecule and a 2 atom molecule.\n",
    "    \"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        atom_matrix, node_graph_indices = inputs\n",
    "        return self._reducer(atom_matrix, node_graph_indices)\n",
    "\n",
    "class ReduceBondToPro(Reducer):\n",
    "    \"\"\"\n",
    "    Sums over bonds acoording to bond_index to get target bond properties\n",
    "\n",
    "    Inputs\n",
    "\n",
    "    bond_matrix : (num_bonds_in_batch, d)\n",
    "        bond hidden states for each bond in the batch\n",
    "\n",
    "    bond_index : (bond_atoms_in_batch, )\n",
    "        A scalar for each bond representing which number in the target property\n",
    "        the bond links to. This is generated by the preprocessor class, and\n",
    "        essentially looks like [-1,-1,0,0,-1,-1,1....]\n",
    "    \"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        bond_matrix, bond_index, n_pro = inputs\n",
    "        num_segments = tf.reduce_sum(n_pro)\n",
    "        return self._reducer(bond_matrix, bond_index, num_segments)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 3\n",
    "        return input_shape[0]\n",
    "\n",
    "class ReduceAtomToPro(Reducer):\n",
    "    \"\"\"\n",
    "    Sums over atoms acoording to atom_index to get target atom properties\n",
    "\n",
    "    Inputs\n",
    "\n",
    "    atom_matrix : (num_atoms_in_batch, d)\n",
    "        atom hidden states for each atom in the batch\n",
    "\n",
    "    atom_index : (atom_atoms_in_batch, )\n",
    "        A scalar for each atom representing which number in the target property\n",
    "        the atom links to. This is generated by the preprocessor class, and\n",
    "        essentially looks like [-1,-1,0,0,-1,-1,1....]\n",
    "    \"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        atom_matrix, atom_index, n_pro = inputs\n",
    "        num_segments = tf.reduce_sum(n_pro)\n",
    "        return self._reducer(atom_matrix, atom_index, num_segments)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 3\n",
    "        return input_shape[0]\n",
    "\n",
    "class ReduceBondToAtom(Reducer):\n",
    "\n",
    "    \"\"\" Sums over the incoming messages from all sender atoms.\n",
    "\n",
    "    Inputs:\n",
    "\n",
    "    bond_matrix : (num_bonds_in_batch, d)\n",
    "        A matrix of messages coming from each sender atom; one row for each\n",
    "        bond/edge.\n",
    "\n",
    "    connectivity : (num_bonds_in_batch, 2)\n",
    "        A matrix of (a_i, a_j) pairs that indicates the bond in bond_matrix\n",
    "        connecting atom_matrix[a_j] to atom_matrix[a_i].\n",
    "        The first entry indicates the recieving atom.\n",
    "\n",
    "    Again, I'm careful to only have the messages be a function of the sending\n",
    "    node, such that we can use sorted methods in performing the reduction.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        bond_matrix, connectivity = inputs\n",
    "        return self._reducer(bond_matrix, connectivity[:, 0])\n",
    "\n",
    "\n",
    "class Squeeze(Layer):\n",
    "    \"\"\" Keras forces inputs to be a vector per entry, so this layer squeezes\n",
    "    them to a single dimension.\n",
    "\n",
    "    I.e., node_graph_indices will have shape (num_atoms_in_batch, 1), while its\n",
    "    easier to work with a vector of shape (num_atoms_in_batch,)\n",
    "    \"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return K.squeeze(inputs, 1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1]\n",
    "\n",
    "\n",
    "class Embedding2D(Layer):\n",
    "    \"\"\" Keras typically wants to embed items as a single vector, while for the\n",
    "    matrix multiplication method of Gilmer 2017 we need a matrix for each bond\n",
    "    type. This just implements that fairly simple extension of the traditional\n",
    "    embedding layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim,\n",
    "                 embeddings_initializer='uniform',\n",
    "                 embeddings_regularizer=None,\n",
    "                 embeddings_constraint=None,\n",
    "                 **kwargs):\n",
    "\n",
    "        super(Embedding2D, self).__init__(**kwargs)\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.embeddings_initializer = initializers.get(embeddings_initializer)\n",
    "        self.embeddings_regularizer = regularizers.get(embeddings_regularizer)\n",
    "        self.embeddings_constraint = constraints.get(embeddings_constraint)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        self.embeddings = self.add_weight(\n",
    "            shape=(self.input_dim, self.output_dim, self.output_dim),\n",
    "            initializer=self.embeddings_initializer,\n",
    "            name='bond_embedding_weights',\n",
    "            regularizer=self.embeddings_regularizer,\n",
    "            constraint=self.embeddings_constraint)\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.nn.embedding_lookup(self.embeddings, inputs)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim, self.output_dim)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'input_dim': self.input_dim,\n",
    "            'output_dim': self.output_dim,\n",
    "            'embeddings_initializer':\n",
    "            initializers.serialize(self.embeddings_initializer),\n",
    "            'embeddings_regularizer':\n",
    "            regularizers.serialize(self.embeddings_regularizer),\n",
    "            'embeddings_constraint':\n",
    "            constraints.serialize(self.embeddings_constraint),\n",
    "        }\n",
    "        base_config = super(Embedding2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class EdgeNetwork(Layer):\n",
    "    \"\"\" A layer to embed (bond_type, distance) pairs as a NxN matrix.\n",
    "\n",
    "    Inputs:\n",
    "    units : dimension of the output matrix\n",
    "    bond_classes : number of unique bonds\n",
    "\n",
    "    First perfoms a 1-hot encoding of the bond_type, then passes the\n",
    "    (*one_hot_encoding, distance) vector to a dense layer. This is the \"Edge\n",
    "    Network\" message described by Gilmer, 2017.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, units, bond_classes,\n",
    "                 activation=None,\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "\n",
    "        super(EdgeNetwork, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.bond_classes = bond_classes\n",
    "\n",
    "        self.activation = activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        self.kernel = self.add_weight(shape=(self.bond_classes + 1, self.units**2),\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      name='kernel',\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.units**2,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        bond_type, distance = inputs\n",
    "        bond_type_onehot = tf.one_hot(tf.squeeze(bond_type), self.bond_classes)\n",
    "        stacked_inputs = tf.concat([bond_type_onehot, distance], 1)\n",
    "\n",
    "        output = K.dot(stacked_inputs, self.kernel)\n",
    "        if self.use_bias:\n",
    "            output = K.bias_add(output, self.bias, data_format='channels_last')\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "\n",
    "        output = tf.reshape(output, [-1, self.units, self.units])\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], self.units, self.units)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'units': self.units,\n",
    "            'bond_classes': self.bond_classes,\n",
    "            'activation': activations.serialize(self.activation),\n",
    "            'use_bias': self.use_bias,\n",
    "            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
    "            'bias_initializer': initializers.serialize(self.bias_initializer),\n",
    "            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
    "            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "            'activity_regularizer':\n",
    "                regularizers.serialize(self.activity_regularizer),\n",
    "            'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
    "            'bias_constraint': constraints.serialize(self.bias_constraint)\n",
    "        }\n",
    "        base_config = super(EdgeNetwork, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbW4GG4Ibj5C"
   },
   "source": [
    "# File Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "ZO44bbWPTpjd"
   },
   "outputs": [],
   "source": [
    "train_file = 'C:/Users/drjac/OneDrive/CSC461/data/train.pkl.gz' # '/content/drive/MyDrive/CSC461/train.pkl.gz'\n",
    "test_file = 'C:/Users/drjac/OneDrive/CSC461/data/test.pkl.gz' # '/content/drive/MyDrive/CSC461/test.pkl.gz'\n",
    "val_file = 'C:/Users/drjac/OneDrive/CSC461/data/valid.pkl.gz' # '/content/drive/MyDrive/CSC461/valid.pkl.gz'\n",
    "train = pd.read_pickle(train_file)\n",
    "test = pd.read_pickle(test_file)\n",
    "val = pd.read_pickle(val_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S2ARmo4SWPF0",
    "outputId": "20b07400-c6ec-45a3-c9bd-1a84d78f1964"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.2862277,  1.8515768,  3.06077  ,  1.8287755,  2.1147938,\n",
       "        1.9405926,  2.4125774,  5.2048125,  1.7113038,  1.9827293,\n",
       "        1.6478252,  5.1483564,  9.646486 ,  6.1986713,  2.1315756,\n",
       "        2.0812304,  2.3436267,  1.7480594,  2.5291371,  1.9526317,\n",
       "        5.372994 ,  1.7547172,  1.7854533,  2.181191 , 11.188395 ,\n",
       "        6.192834 ], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = train.Shift.values\n",
    "y_test = test.Shift.values\n",
    "y_val = val.Shift.values\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M3V_g2ZsWQuS",
    "outputId": "4307d622-2b4b-42a0-c6ab-61405831944c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_atom': 57,\n",
       " 'n_bond': 1274,\n",
       " 'n_pro': 26,\n",
       " 'atom': array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 3, 2, 2, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=int64),\n",
       " 'bond': array([2, 3, 3, ..., 0, 0, 0], dtype=int64),\n",
       " 'distance': array([1.0817016, 1.3787208, 1.3937682, ..., 4.7569475, 4.868445 ,\n",
       "        4.896678 ], dtype=float32),\n",
       " 'connectivity': array([[ 0, 31],\n",
       "        [ 0,  5],\n",
       "        [ 0,  1],\n",
       "        ...,\n",
       "        [56, 53],\n",
       "        [56, 25],\n",
       "        [56, 54]], dtype=int64),\n",
       " 'atom_index': array([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0,  1,  2,\n",
       "         3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
       "        20, 21, 22, 23, 24, 25], dtype=int64)}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_inputs_path = 'C:/Users/drjac/OneDrive/CSC461/data/processed_inputs.pkl' # '/content/drive/MyDrive/CSC461/processed_inputs.p'\n",
    "\n",
    "# Load the pickle file\n",
    "with open(processed_inputs_path, 'rb') as file:\n",
    "    processed_inputs = pickle.load(file)\n",
    "\n",
    "# Access the elements in the dictionary\n",
    "inputs_train = processed_inputs['inputs_train']\n",
    "inputs_valid = processed_inputs['inputs_valid']\n",
    "inputs_test = processed_inputs['inputs_test']\n",
    "preprocessor = processed_inputs['preprocessor']\n",
    "\n",
    "inputs_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "67kRJ4mT4N6r"
   },
   "outputs": [],
   "source": [
    "with open(processed_inputs_path, 'rb') as f:\n",
    "    input_data = pickle.load(f)\n",
    "\n",
    "preprocessor = input_data['preprocessor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nj3t2qmFnveC"
   },
   "source": [
    "# More functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "6Nm4lRNQ6ha5"
   },
   "outputs": [],
   "source": [
    "def rbf_expansion(distances, mu=0, delta=0.1, kmax=256):\n",
    "    k = np.arange(0, kmax)\n",
    "    logits = -(np.atleast_2d(distances).T - (-mu + delta * k))**2 / delta\n",
    "    return np.exp(logits)\n",
    "\n",
    "def atomic_number_tokenizer(atom):\n",
    "    return atom.GetNumRadicalElectrons()\n",
    "\n",
    "def _compute_stacked_offsets(sizes, repeats):\n",
    "    return np.repeat(np.cumsum(np.hstack([0, sizes[:-1]])), repeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "R0vHYfHUn0we"
   },
   "outputs": [],
   "source": [
    "class GraphSequence(Sequence):\n",
    "\n",
    "    def __init__(self, inputs, y=None, batch_size=1, shuffle=True,\n",
    "                 final_batch=True):\n",
    "        \"\"\" A keras.Sequence generator to be passed to model.fit_generator. (or\n",
    "        any other *_generator method.) Returns (inputs, y) tuples where\n",
    "        molecule feature matrices have been stitched together. Offsets the\n",
    "        connectivity matrices such that atoms are indexed appropriately.\n",
    "\n",
    "        batch_size: number of molecules per batch\n",
    "        shuffle : whether to shuffle the input data\n",
    "        final_batch : whether to include the final, incomplete batch\n",
    "\n",
    "        \"\"\"\n",
    "        self._inputs = inputs\n",
    "        self._y = np.asarray(y) if y is not None else None\n",
    "        self._input_keys = list(inputs[0].keys())\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.final_batch = final_batch\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Total number of batches \"\"\"\n",
    "        if self.final_batch:\n",
    "            return int(np.ceil(len(self._inputs) / float(self.batch_size)))\n",
    "        else:\n",
    "            return int(np.floor(len(self._inputs) / float(self.batch_size)))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            indices = np.arange(0, len(self._inputs))\n",
    "            np.random.shuffle(indices)\n",
    "            self._inputs = [self._inputs[i] for i in indices]\n",
    "            if self._y is not None:\n",
    "                self._y = self._y[indices]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Calculate the feature matrices for a whole batch (with index `i` <\n",
    "        self.__len__). This involves adding offsets to the indices for each\n",
    "        atom in the connectivity matrix; such that atoms and bonds in later\n",
    "        molecules still refer to the correct atoms.\n",
    "\n",
    "        \"\"\"\n",
    "        batch_indexes = idx * self.batch_size + np.arange(0, self.batch_size)\n",
    "        batch_indexes = batch_indexes[batch_indexes < len(self._inputs)]\n",
    "\n",
    "        batch_data = {\n",
    "            key: self._concat([self._inputs[i][key] for i in batch_indexes])\n",
    "            for key in self._input_keys}\n",
    "\n",
    "        # Offset the connectivity matrix to account for the multiple graphs per\n",
    "        # batch\n",
    "        offset = _compute_stacked_offsets(\n",
    "            batch_data['n_atom'], batch_data['n_bond'])\n",
    "\n",
    "        batch_data['connectivity'] += offset[:, np.newaxis]\n",
    "\n",
    "        # Compute graph indices with shape (n_atom,) that indicate to which\n",
    "        # molecule each atom belongs.\n",
    "        n_graphs = len(batch_indexes)\n",
    "        batch_data['node_graph_indices'] = np.repeat(\n",
    "            np.arange(n_graphs), batch_data['n_atom'])\n",
    "\n",
    "        batch_data = self.process_data(batch_data)\n",
    "\n",
    "        # Keras takes to options, one (x, y) pairs, or just (x,) pairs if we're\n",
    "        # doing predictions. Here, if we've specified a y matrix, we return the\n",
    "        # x,y pairs for training, otherwise just return the x data.\n",
    "        if self._y is not None:\n",
    "            return (batch_data, np.concatenate(self._y[batch_indexes]).reshape(-1,1))\n",
    "\n",
    "        else:\n",
    "            return batch_data\n",
    "\n",
    "    def process_data(self, batch_data):\n",
    "        \"\"\" function to add additional processing to batch data before returning \"\"\"\n",
    "\n",
    "        # These aren't used currently, so I pop them. But we might need them at\n",
    "        # a later time.\n",
    "        del batch_data['n_atom']\n",
    "        del batch_data['n_bond']\n",
    "\n",
    "        return batch_data\n",
    "\n",
    "\n",
    "    def _concat(self, to_stack):\n",
    "        \"\"\" function to stack (or concatentate) depending on dimensions \"\"\"\n",
    "\n",
    "        if np.asarray(to_stack[0]).ndim >= 2:\n",
    "            return np.concatenate(to_stack)\n",
    "\n",
    "        else:\n",
    "            return np.hstack(to_stack)\n",
    "\n",
    "\n",
    "def _compute_stacked_offsets(sizes, repeats):\n",
    "    \"\"\" Computes offsets to add to indices of stacked np arrays.\n",
    "    When a set of np arrays are stacked, the indices of those from the second on\n",
    "    must be offset in order to be able to index into the stacked np array. This\n",
    "    computes those offsets.\n",
    "\n",
    "    Args:\n",
    "        sizes: A 1D sequence of np arrays of the sizes per graph.\n",
    "        repeats: A 1D sequence of np arrays of the number of repeats per graph.\n",
    "    Returns:\n",
    "        The index offset per graph.\n",
    "    \"\"\"\n",
    "    return np.repeat(np.cumsum(np.hstack([0, sizes[:-1]])), repeats)\n",
    "\n",
    "class RBFSequence(GraphSequence):\n",
    "    def process_data(self, batch_data):\n",
    "        batch_data['distance_rbf'] = rbf_expansion(batch_data['distance'])\n",
    "\n",
    "        offset = _compute_stacked_offsets(\n",
    "            batch_data['n_pro'], batch_data['n_atom'])\n",
    "\n",
    "        offset = np.where(batch_data['atom_index']>=0, offset, 0)\n",
    "        batch_data['atom_index'] += offset\n",
    "\n",
    "        del batch_data['n_atom']\n",
    "        del batch_data['n_bond']\n",
    "        del batch_data['distance']\n",
    "\n",
    "        return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "7JZ0yuQYoUmi"
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "for row,y in zip(input_data['inputs_train'], y_train):\n",
    "    X.extend(row['atom'][row['atom_index']>=0])\n",
    "    Y.extend(y[row['atom_index'][row['atom_index']>=0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EW2gVTxhp9Sy",
    "outputId": "53e60b87-68c7-4418-c0b1-ae8fb08ab2b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102199"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atom_total = 0\n",
    "for mol in inputs_train:\n",
    "  atom_total += mol['n_pro']\n",
    "atom_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1kLYzN9NqEyF",
    "outputId": "e4b32369-ed5a-4f96-b615-7d5eee68ad3c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.2862277,  1.8515768,  3.06077  ,  1.8287755,  2.1147938,\n",
       "        1.9405926,  2.4125774,  5.2048125,  1.7113038,  1.9827293,\n",
       "        1.6478252,  5.1483564,  9.646486 ,  6.1986713,  2.1315756,\n",
       "        2.0812304,  2.3436267,  1.7480594,  2.5291371,  1.9526317,\n",
       "        5.372994 ,  1.7547172,  1.7854533,  2.181191 , 11.188395 ,\n",
       "        6.192834 ], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data['inputs_train'][0]\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v0Z0GuvVq1g_",
    "outputId": "e3c2b412-a518-4e5e-d8bc-470bfe289a00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8515768"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OxDZrrrMsbeb",
    "outputId": "367d6e31-39f2-4877-d283-73dfec5ea3e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OFimL663sevi",
    "outputId": "ecfe60d6-886e-4f50-add6-a905de991fff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_atom': 57,\n",
       " 'n_bond': 1274,\n",
       " 'n_pro': 26,\n",
       " 'atom': array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 3, 2, 2, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=int64),\n",
       " 'bond': array([2, 3, 3, ..., 0, 0, 0], dtype=int64),\n",
       " 'distance': array([1.0817016, 1.3787208, 1.3937682, ..., 4.7569475, 4.868445 ,\n",
       "        4.896678 ], dtype=float32),\n",
       " 'connectivity': array([[ 0, 31],\n",
       "        [ 0,  5],\n",
       "        [ 0,  1],\n",
       "        ...,\n",
       "        [56, 53],\n",
       "        [56, 25],\n",
       "        [56, 54]], dtype=int64),\n",
       " 'atom_index': array([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0,  1,  2,\n",
       "         3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
       "        20, 21, 22, 23, 24, 25], dtype=int64)}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "QPhHEqaGshW1"
   },
   "outputs": [],
   "source": [
    "atom_means = pd.DataFrame({'atom':X, 'shift':Y}).dropna().groupby('atom')['shift'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "id": "OjDQKKgss0EH",
    "outputId": "b51114c5-22be-4c9e-fdf3-cc10dff01fcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "atom\n",
       "0    0.000000\n",
       "1    0.000000\n",
       "2    3.022405\n",
       "3    3.280265\n",
       "4    3.952668\n",
       "5    0.000000\n",
       "6    0.000000\n",
       "7    1.258198\n",
       "8    0.000000\n",
       "9    0.000000\n",
       "Name: shift, dtype: float32"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atom_means = atom_means.reindex(np.arange(preprocessor.atom_classes)).fillna(0)\n",
    "atom_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "skHK0k8ms1bt",
    "outputId": "6d8c51a6-179c-4f53-d9c5-debdeacdcafd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "atom\n",
       "4    102154\n",
       "2        31\n",
       "3        13\n",
       "7         1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'atom':X, 'shift':Y})['atom'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "t87Mnm-is8Z8"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_sequence = RBFSequence(input_data['inputs_train'], y_train, batch_size)\n",
    "valid_sequence = RBFSequence(input_data['inputs_valid'], y_val, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 186
    },
    "id": "YRPVY8mWxdxA",
    "outputId": "4d5d4bd2-e25f-4f12-8200-3733c4ed8444"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.RBFSequence"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(train_sequence)\n",
    "train_sequence.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5CFxre_ixnXj",
    "outputId": "441a5747-1b81-45c3-fe1f-e093c465909d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_atom': 57,\n",
       " 'n_bond': 1274,\n",
       " 'n_pro': 26,\n",
       " 'atom': array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 3, 2, 2, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=int64),\n",
       " 'bond': array([2, 3, 3, ..., 0, 0, 0], dtype=int64),\n",
       " 'distance': array([1.0817016, 1.3787208, 1.3937682, ..., 4.7569475, 4.868445 ,\n",
       "        4.896678 ], dtype=float32),\n",
       " 'connectivity': array([[ 0, 31],\n",
       "        [ 0,  5],\n",
       "        [ 0,  1],\n",
       "        ...,\n",
       "        [56, 53],\n",
       "        [56, 25],\n",
       "        [56, 54]], dtype=int64),\n",
       " 'atom_index': array([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0,  1,  2,\n",
       "         3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
       "        20, 21, 22, 23, 24, 25], dtype=int64),\n",
       " 'distance_rbf': array([[8.28731687e-006, 6.52437027e-005, 4.20537142e-004, ...,\n",
       "         0.00000000e+000, 0.00000000e+000, 0.00000000e+000],\n",
       "        [5.55421160e-009, 7.92017194e-008, 9.24672414e-007, ...,\n",
       "         0.00000000e+000, 0.00000000e+000, 0.00000000e+000],\n",
       "        [3.65965223e-009, 5.37801536e-008, 6.47061206e-007, ...,\n",
       "         0.00000000e+000, 0.00000000e+000, 0.00000000e+000],\n",
       "        ...,\n",
       "        [5.31443856e-099, 6.51418454e-095, 6.53738148e-091, ...,\n",
       "         0.00000000e+000, 0.00000000e+000, 0.00000000e+000],\n",
       "        [1.16033091e-103, 1.77758321e-099, 2.22955979e-095, ...,\n",
       "         0.00000000e+000, 0.00000000e+000, 0.00000000e+000],\n",
       "        [7.36606217e-105, 1.19400564e-100, 1.58459571e-096, ...,\n",
       "         0.00000000e+000, 0.00000000e+000, 0.00000000e+000]])}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data = input_data['inputs_train'][0]\n",
    "\n",
    "batch_data['distance_rbf'] = rbf_expansion(batch_data['distance'])\n",
    "# offset = _compute_stacked_offsets(batch_data['n_pro'], batch_data['n_atom'])\n",
    "# sizes = batch_data['n_atom']\n",
    "# repeats = batch_data['n_bond']\n",
    "# np.repeat(np.cumsum(np.hstack([0, sizes[:-1]])), repeats)\n",
    "batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uyh518xlGlJG",
    "outputId": "9c265f79-e4f5-4acd-cf7b-44c1a34faefb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sequence[2][0]['n_pro'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hHCVeD1cQXr"
   },
   "source": [
    "# IMPORTANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fML0nq9iGo7T",
    "outputId": "3c6cf93a-7b5e-45d6-db3d-7141dbb78e60"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_pro': array([23, 33, 22, 24, 16, 20, 20, 29, 31,  4, 26, 21, 30, 20, 22, 22, 21,\n",
       "         7, 13, 28, 39, 20, 36, 11, 24, 18, 14, 30, 40, 16, 27, 44]),\n",
       " 'atom': array([2, 2, 2, ..., 4, 4, 4], dtype=int64),\n",
       " 'bond': array([2, 2, 2, ..., 0, 0, 0], dtype=int64),\n",
       " 'connectivity': array([[   0,   22],\n",
       "        [   0,   23],\n",
       "        [   0,   24],\n",
       "        ...,\n",
       "        [1500, 1445],\n",
       "        [1500, 1492],\n",
       "        [1500, 1447]], dtype=int64),\n",
       " 'atom_index': array([ -1,  -1,  -1, ..., 748, 749, 750], dtype=int64),\n",
       " 'node_graph_indices': array([ 0,  0,  0, ..., 31, 31, 31]),\n",
       " 'distance_rbf': array([[7.14868225e-006, 5.70513607e-005, 3.72775296e-004, ...,\n",
       "         0.00000000e+000, 0.00000000e+000, 0.00000000e+000],\n",
       "        [6.49087834e-006, 5.22621210e-005, 3.44517825e-004, ...,\n",
       "         0.00000000e+000, 0.00000000e+000, 0.00000000e+000],\n",
       "        [6.45558713e-006, 5.20039009e-005, 3.42986638e-004, ...,\n",
       "         0.00000000e+000, 0.00000000e+000, 0.00000000e+000],\n",
       "        ...,\n",
       "        [1.92554872e-098, 2.29713828e-094, 2.24367976e-090, ...,\n",
       "         0.00000000e+000, 0.00000000e+000, 0.00000000e+000],\n",
       "        [1.74600082e-099, 2.19077055e-095, 2.25055924e-091, ...,\n",
       "         0.00000000e+000, 0.00000000e+000, 0.00000000e+000],\n",
       "        [3.66879193e-106, 6.32143086e-102, 8.91761679e-098, ...,\n",
       "         0.00000000e+000, 0.00000000e+000, 0.00000000e+000]])}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequence[2][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "maYTp0a2cTtS"
   },
   "source": [
    "# IMPORTANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "ODp2qXCOuH9e"
   },
   "outputs": [],
   "source": [
    "atom_index = Input(shape=(1,), name='atom_index', dtype='int32')\n",
    "atom_types = Input(shape=(1,), name='atom', dtype='int32')\n",
    "distance_rbf = Input(shape=(256,), name='distance_rbf', dtype='float32')\n",
    "connectivity = Input(shape=(2,), name='connectivity', dtype='int32')\n",
    "n_pro = Input(shape=(1,), name='n_pro', dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TDNvGfOs1s03",
    "outputId": "b542e7e1-d3d1-42cb-82f4-a1aa1bca0744"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'atom_index')>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atom_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2uR6hlOcWac"
   },
   "source": [
    "# IMPORTANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "2cC3mCYn48_7"
   },
   "outputs": [],
   "source": [
    "squeeze = Squeeze()\n",
    "\n",
    "satom_index = squeeze(atom_index)\n",
    "satom_types = squeeze(atom_types)\n",
    "sn_pro = squeeze(n_pro)\n",
    "# Initialize RNN and MessageLayer instances\n",
    "atom_features = 256\n",
    "\n",
    "# Initialize the atom states\n",
    "atom_state = Embedding(\n",
    "    preprocessor.atom_classes,\n",
    "    atom_features, name='atom_embedding')(satom_types)\n",
    "\n",
    "atomwise_shift = Embedding(\n",
    "    preprocessor.atom_classes, 1, name='atomwise_shift',\n",
    "    embeddings_initializer=keras.initializers.constant(atom_means.values)\n",
    ")(satom_types)\n",
    "\n",
    "bond_state = distance_rbf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClW8nxh9cYH9"
   },
   "source": [
    "# IMPORTANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "__MNsp1Q587O"
   },
   "outputs": [],
   "source": [
    "def message_block(atom_state, bond_state, connectivity):\n",
    "\n",
    "    atom_state = Dense(atom_features, use_bias=False)(atom_state)\n",
    "\n",
    "    source_atom_gather = GatherAtomToBond(1)\n",
    "    target_atom_gather = GatherAtomToBond(0)\n",
    "\n",
    "    source_atom = source_atom_gather([atom_state, connectivity])\n",
    "    target_atom = target_atom_gather([atom_state, connectivity])\n",
    "\n",
    "    # Edge update network\n",
    "    bond_state_message = Concatenate()([source_atom, target_atom, bond_state])\n",
    "    bond_state_message = Dense(2*atom_features, activation='softplus')(bond_state_message)\n",
    "    bond_state_message = Dense(atom_features)(bond_state_message)\n",
    "\n",
    "    bond_state_message = Dense(atom_features, activation='softplus')(bond_state_message)\n",
    "    bond_state_message = Dense(atom_features, activation='softplus')(bond_state_message)\n",
    "    bond_state = Add()([bond_state_message, bond_state])\n",
    "\n",
    "    # message function\n",
    "    messages = Multiply()([source_atom, bond_state])\n",
    "    messages = ReduceBondToAtom(reducer='sum')([messages, connectivity])\n",
    "\n",
    "    # state transition function\n",
    "    messages = Dense(atom_features, activation='softplus')(messages)\n",
    "    messages = Dense(atom_features)(messages)\n",
    "    atom_state = Add()([atom_state, messages])\n",
    "\n",
    "    return atom_state, bond_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zIrdpC596IAv",
    "outputId": "3aa3f6da-25bb-4a5d-ebff-58ff6cbd4237"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'distance_rbf')>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bond_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVUqI6g7cdJD"
   },
   "source": [
    "# IMPORTANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "83nRXHg27_ml"
   },
   "outputs": [],
   "source": [
    "for _ in range(3):\n",
    "    atom_state, bond_state = message_block(atom_state, bond_state, connectivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlDq_wBLPeKY"
   },
   "source": [
    "# Going through test forward pass of message passing in the CASCADE model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nid7LPsTcfDf"
   },
   "source": [
    "# IMPORTANT (INSPECTION OF MESSAGE BLOCK FUNCTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4cAXTrrHQFvb",
    "outputId": "87a4a9fe-beb5-4747-acbf-2f6d3b3cebd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n",
      "(None, 256) = (None, 1)\n",
      "(None, 256)\n"
     ]
    }
   ],
   "source": [
    "#Testing Keras Embedding Layer to see whats happening (go from atom label to 256 vector)\n",
    "test_atom_state = Embedding(\n",
    "    preprocessor.atom_classes,\n",
    "    atom_features, name='atom_embedding')\n",
    "model = keras.Sequential()\n",
    "model.add(test_atom_state)\n",
    "input = np.array([2,2])\n",
    "test_atom_vectors = model.predict(input)\n",
    "\n",
    "#Defining connectivity, bond state for a simple one connection molecule\n",
    "test_connectivity = tf.convert_to_tensor(np.array(([0,1],[1,0])))\n",
    "test_bond_state = rbf_expansion(np.array([5,5]))\n",
    "\n",
    "#One Dense layer is run on the atoms initially\n",
    "test_atom_vectors = Dense(atom_features, use_bias=False)(test_atom_vectors)\n",
    "\n",
    "#Gather target and source atoms for messaging\n",
    "gather_thingy = GatherAtomToBond(0)\n",
    "other_gather_thingy = GatherAtomToBond(1)\n",
    "target_atoms = gather_thingy(inputs = [test_atom_vectors,test_connectivity])\n",
    "source_atoms = other_gather_thingy(inputs = [test_atom_vectors,test_connectivity])\n",
    "\n",
    "#Concatenates target, source and bond vectors into one long vector (3*256 in this case)\n",
    "bond_message = Concatenate()([target_atoms, source_atoms, test_bond_state])\n",
    "\n",
    "#Dense layer to 512 length vector\n",
    "bond_message = Dense(2*atom_features, activation='softplus')(bond_message)\n",
    "\n",
    "#Dense layer back to 256 length vector\n",
    "bond_message = Dense(atom_features)(bond_message)\n",
    "\n",
    "# Adding the message to the existing bond value and getting updated value\n",
    "test_bond_state = Add()([bond_message, test_bond_state])\n",
    "\n",
    "# Working on updating nodes, starting with element wise multiplication of the source atoms and the edges\n",
    "node_message = Multiply()([source_atoms, test_bond_state])\n",
    "\n",
    "# Adds all bonds connecting a single atom together\n",
    "node_message = ReduceBondToAtom(reducer='sum')([node_message, connectivity])\n",
    "\n",
    "# Two dense layers\n",
    "node_message = Dense(atom_features, activation='softplus')(node_message)\n",
    "node_message = Dense(atom_features)(node_message)\n",
    "\n",
    "# Adding the node messages to get the final node values\n",
    "final_atom_vectors = Add()([atom_state, node_message])\n",
    "print(final_atom_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TONqqVuV9Jqb"
   },
   "source": [
    "# IMPORTANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "qckQnwi38Lyl"
   },
   "outputs": [],
   "source": [
    "atom_state = ReduceAtomToPro(reducer='unsorted_mean')([atom_state, satom_index, sn_pro])\n",
    "atomwise_shift = ReduceAtomToPro(reducer='unsorted_mean')([atomwise_shift, satom_index, sn_pro])\n",
    "\n",
    "atom_state = Dense(atom_features, activation='softplus')(atom_state)\n",
    "atom_state = Dense(atom_features, activation='softplus')(atom_state)\n",
    "atom_state = Dense(atom_features//2, activation='softplus')(atom_state)\n",
    "atom_state = Dense(1)(atom_state)\n",
    "\n",
    "output = Add()([atom_state, atomwise_shift])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VhuwA5wkG-QP",
    "outputId": "952f7db8-248f-401e-8749-7d86773020c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'atom_index')>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atom_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3zg1FLbcv5d"
   },
   "source": [
    "# IMPORTANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "f4W7Q-KB8Tjj",
    "outputId": "243a6e5c-c6ea-4107-a1a9-1c4ea5aa5fc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"graph_model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " atom (InputLayer)              [(None, 1)]          0           []                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " distance_rbf (InputLayer)      [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " connectivity (InputLayer)      [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " n_pro (InputLayer)             [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " atom_index (InputLayer)        [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lr = 5E-4\n",
    "epochs = 1200\n",
    "\n",
    "model = GraphModel([\n",
    "        atom_index, atom_types, distance_rbf, connectivity, n_pro], [atom_index])\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), loss='mae')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQ2Syo-TcyrZ"
   },
   "source": [
    "# IMPORTANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "hjrBuqqt8cTE"
   },
   "outputs": [],
   "source": [
    "def decay_fn(epoch, learning_rate):\n",
    "    \"\"\" Jorgensen decays to 0.96*lr every 100,000 batches, which is approx\n",
    "    every 28 epochs \"\"\"\n",
    "    if (epoch % 70) == 0:\n",
    "        return 0.96 * learning_rate\n",
    "    else:\n",
    "        return learning_rate\n",
    "\n",
    "lr_decay = LearningRateScheduler(decay_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kH6hYdbBc2hv"
   },
   "source": [
    "## IMPORTANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jLoC7MKrfPva",
    "outputId": "d23730d6-5ca5-4ab0-812f-dfaa9d408ac4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(676, 1)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequence[0][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "cnogPwEG95OY",
    "outputId": "e84bc491-20cc-4fd9-90df-bf4e75a3df9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\drjac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\functional.py:639: UserWarning: Input dict contained keys ['bond', 'node_graph_indices'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'mean_absolute_error/sub' defined at (most recent call last):\n    File \"c:\\Users\\drjac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\drjac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\drjac\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\drjac\\AppData\\Roaming\\Python\\Python310\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"C:\\Users\\drjac\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"C:\\Users\\drjac\\AppData\\Roaming\\Python\\Python310\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\drjac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 595, in run_forever\n      self._run_once()\n    File \"c:\\Users\\drjac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1881, in _run_once\n      handle._run()\n    File \"c:\\Users\\drjac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\drjac\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\drjac\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\drjac\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"C:\\Users\\drjac\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"C:\\Users\\drjac\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\drjac\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\drjac\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\drjac\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\drjac\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\drjac\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\drjac\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\drjac\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\drjac\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\drjac\\AppData\\Local\\Temp\\ipykernel_7524\\2934220159.py\", line 4, in <module>\n      hist = model.fit(train_sequence,\n    File \"c:\\Users\\drjac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\drjac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\drjac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\drjac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\drjac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\drjac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\drjac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1109, in compute_loss\n      return self.compiled_loss(\n    File \"c:\\Users\\drjac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\drjac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 142, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\drjac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 268, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\drjac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 1611, in mean_absolute_error\n      return backend.mean(tf.abs(y_pred - y_true), axis=-1)\nNode: 'mean_absolute_error/sub'\nIncompatible shapes: [1493,1] vs. [719,1]\n\t [[{{node mean_absolute_error/sub}}]] [Op:__inference_train_function_3527]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[100], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m csv_logger \u001b[38;5;241m=\u001b[39m CSVLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/CSC461/log.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m ModelCheckpoint(filepath, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, save_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m hist \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_sequence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sequence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_logger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_decay\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\drjac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\drjac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'mean_absolute_error/sub' defined at (most recent call last):\n    File \"c:\\Users\\drjac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\drjac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\drjac\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\drjac\\AppData\\Roaming\\Python\\Python310\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"C:\\Users\\drjac\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"C:\\Users\\drjac\\AppData\\Roaming\\Python\\Python310\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\drjac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 595, in run_forever\n      self._run_once()\n    File \"c:\\Users\\drjac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1881, in _run_once\n      handle._run()\n    File \"c:\\Users\\drjac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\drjac\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\drjac\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\drjac\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"C:\\Users\\drjac\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"C:\\Users\\drjac\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\drjac\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\drjac\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\drjac\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\drjac\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\drjac\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\drjac\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\drjac\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\drjac\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\drjac\\AppData\\Local\\Temp\\ipykernel_7524\\2934220159.py\", line 4, in <module>\n      hist = model.fit(train_sequence,\n    File \"c:\\Users\\drjac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\drjac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\drjac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\drjac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\drjac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\drjac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\drjac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1109, in compute_loss\n      return self.compiled_loss(\n    File \"c:\\Users\\drjac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\drjac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 142, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\drjac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 268, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\drjac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 1611, in mean_absolute_error\n      return backend.mean(tf.abs(y_pred - y_true), axis=-1)\nNode: 'mean_absolute_error/sub'\nIncompatible shapes: [1493,1] vs. [719,1]\n\t [[{{node mean_absolute_error/sub}}]] [Op:__inference_train_function_3527]"
     ]
    }
   ],
   "source": [
    "filepath = \"/content/drive/MyDrive/CSC461/best_model.keras\"\n",
    "csv_logger = CSVLogger('/content/drive/MyDrive/CSC461/log.csv')\n",
    "checkpoint = ModelCheckpoint(filepath, save_best_only=True, save_freq='epoch', verbose=1)\n",
    "hist = model.fit(train_sequence,\n",
    "                 validation_data=valid_sequence,\n",
    "                 epochs=epochs, verbose=1,\n",
    "                 callbacks=[checkpoint, csv_logger, lr_decay])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rn0HUAsP-Fb5",
    "outputId": "01317a86-e8a2-4d89-8c63-2e915e8b3d15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16770, 256)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_sequence[0][0]['distance_rbf'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "anxuJ1x7GFzt",
    "outputId": "747d7f79-51c8-488f-95b5-7bd2944a8cdb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │          \u001b[38;5;34m65,792\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> (257.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m65,792\u001b[0m (257.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> (257.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m65,792\u001b[0m (257.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1037/1037\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.75749016, 0.57033974, 0.67821103, ..., 0.66581386, 0.7193932 ,\n",
       "        0.80530006],\n",
       "       [0.7468433 , 0.56934005, 0.6820942 , ..., 0.63832223, 0.77921623,\n",
       "        0.79469776],\n",
       "       [0.7437823 , 0.57471913, 0.6855024 , ..., 0.64088094, 0.78454655,\n",
       "        0.79514563],\n",
       "       ...,\n",
       "       [0.6949312 , 0.6935917 , 0.6204102 , ..., 0.6970189 , 0.6164915 ,\n",
       "        0.6758731 ],\n",
       "       [0.68610764, 0.6947843 , 0.6158033 , ..., 0.70515215, 0.6292391 ,\n",
       "        0.6921318 ],\n",
       "       [0.6856468 , 0.6948941 , 0.61546797, ..., 0.7058479 , 0.63004595,\n",
       "        0.69348747]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mod = keras.Sequential()\n",
    "test_mod.add(Input(shape=(256,), name='distance_rbf', dtype='float32'))\n",
    "test_mod.add(Dense(256, activation='softplus'))\n",
    "test_mod.summary()\n",
    "test_mod.predict(train_sequence[0][0]['distance_rbf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lUi0daTXPvUi",
    "outputId": "fbcf10b1-e460-40da-965a-d0b1a8f21d1b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_pro': array([14, 13, 36, 10, 28, 12, 17, 10, 16, 10,  9, 15, 14, 20, 12, 11, 13,\n",
       "         8, 12, 14, 22, 14, 12, 10, 17, 15, 18, 10, 19, 16, 15, 12]),\n",
       " 'atom': array([8, 3, 6, ..., 4, 4, 4]),\n",
       " 'bond': array([53,  0,  0, ...,  0,  0,  0]),\n",
       " 'connectivity': array([[   0,   21],\n",
       "        [   0,   20],\n",
       "        [   0,   19],\n",
       "        ...,\n",
       "        [1027, 1024],\n",
       "        [1027, 1021],\n",
       "        [1027, 1025]]),\n",
       " 'atom_index': array([ -1,  -1,  -1, ..., 471, 472, 473]),\n",
       " 'node_graph_indices': array([ 0,  0,  0, ..., 31, 31, 31]),\n",
       " 'distance_rbf': array([[1.55789473e-08, 2.05925188e-07, 2.22854771e-06, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [1.40757371e-24, 1.38224368e-22, 1.11132012e-20, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [1.31281157e-24, 1.29302538e-22, 1.04268424e-20, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        ...,\n",
       "        [5.29341528e-94, 5.07801862e-90, 3.98835415e-86, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [2.97552958e-97, 3.35006743e-93, 3.08804719e-89, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [6.28503377e-98, 7.31299132e-94, 6.96664377e-90, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequence[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsMl6VHeSVmd"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
